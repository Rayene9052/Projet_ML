# üöó Car Price Prediction ‚Äî Full Project Guide

> README pr√™t √† pusher sur GitHub ‚Äî contenu en fran√ßais, organis√© et avec tous les blocs de code Python en triple backticks.  
> Copie-colle directement ce fichier dans `README.md`.

---

## Table des mati√®res
1. [Vue d‚Äôensemble du workflow](#1-vue-densemble-du-workflow)  
2. [Collecte de donn√©es ‚Äî strat√©gie concr√®te](#2-collecte-de-donn√©es--strat√©gie-concr√®te)  
3. [Champs (features) √† extraire](#3-champs-features-√†-extraire)  
4. [Exemple de scraping (BeautifulSoup)](#4-exemple-de-scraping-beautifulsoup)  
5. [Nettoyage & pr√©paration (pandas)](#5-nettoyage--pr√©paration-pandas)  
6. [Feature engineering](#6-feature-engineering)  
7. [Mod√©lisation ‚Äî pipeline exemple (scikit-learn)](#7-mod√©lisation---pipeline-exemple-scikit-learn)  
8. [√âvaluation ‚Äî m√©triques & validation](#8-√©valuation---m√©triques--validation)  
9. [Probl√®mes r√©currents & solutions](#9-probl√®mes-r√©currents--solutions)  
10. [S√©parer les jeux de donn√©es](#10-s√©parer-les-jeux-de-donn√©es)  
11. [D√©ploiement & livraison](#11-d√©ploiement--livraison)  
12. [Plan de travail (sprint simple)](#12-plan-de-travail-sprint-simple)  
13. [Regex utiles pour parser](#13-regex-utiles-pour-parser)  
14. [Bonnes pratiques & recommandations finales](#14-bonnes-pratiques--recommandations-finales)  

---

## 1) Vue d‚Äôensemble du workflow

- **D√©finir l‚Äôobjectif pr√©cis** ‚Äî pr√©dire le prix en **TND** d‚Äôune annonce voiture (occasion / neuve).  
- **Collecte de donn√©es (scraping)** ‚Äî extraire annonces depuis 4‚Äì6 sites prioritaires (ex : `automobile.tn`, `sayarti.tn`, `argusautomobile.tn`, `tayara.tn`, `sparkauto.tn`, `auto-plus.tn`).  
- **Nettoyage & enrichissement** ‚Äî normaliser prix, km, ann√©e, convertir texte en champs.  
- **Analyse exploratoire (EDA)** ‚Äî distributions, corr√©lations, outliers.  
- **Feature engineering** ‚Äî √¢ge = ann√©e actuelle ‚àí ann√©e immat, `log(price)`, groupements, interactions.  
- **Modeling** ‚Äî baselines (Linear/Tree), mod√®les robustes (RandomForest, XGBoost/CatBoost/LightGBM).  
- **Validation & m√©triques** ‚Äî MAE, RMSE, R¬≤; cross-validation.  
- **D√©ploiement** ‚Äî prototype Streamlit / API (Flask/FastAPI) + Docker.  
- **Documentation & limites** ‚Äî biais, prix manquants, fiabilit√© des annonces.

---

## 2) Collecte de donn√©es ‚Äî strat√©gie concr√®te

- **Choisir 4‚Äì6 sites** (parmi tes 12) avec structure stable et annonces nombreuses. Priorit√© :  
  `automobile.tn`, `sayarti.tn`, `tayara.tn`, `sparkauto.tn`, `auto-plus.tn`, `argusautomobile.tn`.  
- **√âchantillonnage** : prototype ‚Üí **~1000‚Äì3000** annonces ; mod√®le solide ‚Üí **‚â•5000** annonces.  
- **Strat√©gie d‚Äô√©chantillonnage** : si site tr√®s grand, prendre N annonces par marque / par page (ex. 5‚Äì20).  
- **√âlectriques** : extraire tous les √©lectriques (dataset plus petit) et les traiter s√©par√©ment ou ajouter `is_electric`.  
- **Respect & √©thique** : v√©rifier `robots.txt`, conditions d‚Äôutilisation ; throttle requests (sleep 1‚Äì3s), user-agent, pagination soign√©e.  
- **Probl√®mes √† surveiller** : annonces sans prix, prix absurdes (ex. "27 malyoum"), prix en devise diff√©rente, annonces dupliqu√©es, donn√©es dans description (parse requis).

---

## 3) Champs (features) √† extraire (prioritaires)

- `price` (TND) ‚Äî **cible**  
- `brand` (Marque)  
- `model` (Mod√®le)  
- `year` (Ann√©e mise en circulation) ‚Üí `age = 2025 - year` (ou ann√©e actuelle)  
- `mileage` (Kilom√©trage en km)  
- `fuel` (Carburant : Essence, Diesel, Electrique, Hybride)  
- `transmission` (Boite : Auto/Manuelle)  
- `body` (Carrosserie : berline, SUV, utilitaire, bus, ambulance)  
- `power` (Puissance fiscale / r√©elle si dispo)  
- `engine_cc` (Cylindr√©e)  
- `doors`, `seats`  
- `color_ext`, `color_int` (si dispo)  
- `seller_type` (particulier, pro, concessionnaire) ‚Äî souvent dans description  
- `condition` (neuf/occasion/reconditionn√©)  
- `location` (ville/region)  
- `images_count`, `has_warranty`, `features_list` (clim, gps‚Ä¶) ‚Äî convertir en flags  
- `is_electric` (flag)  
- `posting_age` (temps depuis publication) si dispo  
- **Tra√ßabilit√©** : garder `source_site`, `scrape_date`, `ad_id`

---

## 4) Exemple de scraping (squelette) avec BeautifulSoup

> **Remarque** : adapter les s√©lecteurs HTML par site. Tester toujours sur 1 page.

```python
import requests
from bs4 import BeautifulSoup
import time
import re
import pandas as pd

HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; CarPriceBot/1.0)"}

def parse_price(text):
    # normalize text like "27 000 DT" -> 27000
    if not text:
        return None
    t = text.replace('\xa0', ' ').replace(',', '').lower()
    m = re.search(r'(\d[\d\s]*)\s*(dt|tnd|dinars|dinar)?', t)
    if m:
        return int(m.group(1).replace(' ', ''))
    return None

def parse_mileage(text):
    if not text:
        return None
    t = text.lower().replace(' ', '')
    m = re.search(r'(\d[\d,\.]*)\s*(km|kilom)', t)
    if m:
        return int(m.group(1).replace(',', '').replace('.', ''))
    return None

def scrape_listing_page(url):
    r = requests.get(url, headers=HEADERS, timeout=15)
    soup = BeautifulSoup(r.text, 'html.parser')
    results = []
    # exemple g√©n√©rique : boucle sur les cartes d'annonce
    for card in soup.select('.ad-card, .listing-item'):
        title = card.select_one('.title, .ad-title')
        price_el = card.select_one('.price')
        link_el = card.select_one('a')
        link = link_el['href'] if link_el else None
        price = parse_price(price_el.get_text() if price_el else None)
        title_text = title.get_text(strip=True) if title else ''
        results.append({'title': title_text, 'price': price, 'link': link})
    return results

def scrape_ad_details(ad_url):
    r = requests.get(ad_url, headers=HEADERS, timeout=15)
    soup = BeautifulSoup(r.text, 'html.parser')
    # Exemples : adapter s√©lecteurs
    title = soup.select_one('h1').get_text(strip=True) if soup.select_one('h1') else ''
    price = parse_price(soup.select_one('.price').get_text() if soup.select_one('.price') else None)
    description = soup.select_one('.description').get_text(separator=' ', strip=True) if soup.select_one('.description') else ''
    # parsing simple des caract√©ristiques
    specs = {}
    for row in soup.select('.specs tr'):
        cols = row.select('td')
        if len(cols) >= 2:
            key = cols[0].get_text(strip=True).lower()
            val = cols[1].get_text(strip=True)
            specs[key] = val
    return {'title': title, 'price': price, 'description': description, 'specs': specs}

# Exemple d'utilisation
if __name__ == '__main__':
    page_url = 'https://www.example.tn/voitures/page-1'
    listings = scrape_listing_page(page_url)
    df_rows = []
    for l in listings[:20]:
        if not l['link']:
            continue
        details = scrape_ad_details(l['link'])
        df_rows.append(details)
        time.sleep(1.5)  # throttle
    df = pd.DataFrame(df_rows)
    print(df.head())
```
## 5) Nettoyage & pr√©paration (pandas)

- Normaliser prix : retirer annonces sans prix (ou garder `price_missing` flag).  
- Convertir texte ‚Üí num√©riques : `mileage`, `engine_cc`, `year`.  
- Dates : convertir `year` ‚Üí `age`.  
- G√©rer missing : imputer par m√©diane ou utiliser mod√®les qui g√®rent NA (CatBoost).  
- Outliers : couper prix `< 500 TND` ou `> 5 000 000 TND` selon contexte ; log-transform du prix souvent utile.  
- Feature textuelle : extraire `seller_type` par regex (mots-cl√©s : particulier, professionnel, concessionnaire, 1ere main).

### Exemple pipeline (pandas)

```python
import numpy as np

df['price'] = df['price'].astype(float)
df = df[df['price'].notna()]   # ou garder mais marquer

df['year'] = df['specs'].apply(lambda s: int(s.get('ann√©e', 0)) if s and s.get('ann√©e') else np.nan)
df['age'] = 2025 - df['year']

df['mileage'] = df['specs'].apply(lambda s: parse_mileage(s.get('kilom√©trage', '')) if s else np.nan)

# fuel doit √™tre une colonne extraite (ex: specs.get('carburant'))
df['is_electric'] = df['fuel'].str.contains('elect', case=False, na=False).astype(int)

df['log_price'] = np.log1p(df['price'])
```
## 6) Feature engineering important

- `age` (mieux que raw `year`)  
- `log_price` comme cible pour r√©duire skew  
- `mileage_per_year = mileage / max(1, age)`  
- `brand_popularity` = fr√©quence d‚Äôapparition par marque  

**Encodages :**  
- `OneHot` pour faible cardinalit√©  
- `Target Encoding` / embeddings pour `model` si cardinalit√© haute  

**Autres features :**  
- `has_images` / `images_count` ‚Äî annonces avec images valent souvent plus  
- `seller_is_dealer` flag  
- `is_electric` s√©par√© ou interaction `brand * is_electric`

---

## 7) Mod√©lisation ‚Äî pipeline exemple (scikit-learn)

- Baselines : `LinearRegression`, `Ridge`, `Lasso`  
- Arbres : `RandomForestRegressor`, `GradientBoostingRegressor`  
- Mod√®les rapides et performants : `XGBoost`, `LightGBM`, `CatBoost` (CatBoost g√®re natif les cat√©goriques & NA)

### Exemple minimal (scikit-learn pipeline)

```python
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.ensemble import RandomForestRegressor
import numpy as np

num_cols = ['age','mileage','engine_cc','power']
cat_cols = ['brand','model','fuel','transmission','body','seller_type']

preproc = ColumnTransformer([
    ('num', StandardScaler(), num_cols),
    ('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), cat_cols),
])

pipe = Pipeline([
    ('pre', preproc),
    ('model', RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1))
])

X = df[num_cols + cat_cols]
y = df['price']  # or 'log_price'

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

pipe.fit(X_train, y_train)
print("Test R2:", pipe.score(X_test, y_test))
```
# üöó Car Price Prediction ‚Äî Sections 8 √† 14 (README.md)

> Contenu pr√™t √† coller dans ton `README.md`. Tous les blocs de code Python sont en triple backticks.

---

## 8) √âvaluation ‚Äî m√©triques & validation

- **MAE (Mean Absolute Error)** ‚Äî interpr√©table en TND (erreur moyenne).  
- **RMSE** ‚Äî p√©nalise fortement les gros √©carts.  
- **R¬≤** ‚Äî proportion de variance expliqu√©e par le mod√®le.

**Remarques :**
- Si tu entra√Ænes sur `log(price)`, reporte les m√©triques sur l‚Äô√©chelle r√©elle en reconvertissant : `pred_real = np.expm1(pred_log)`.  
- **Validation** recommand√©e : Cross-validation (k=5 ou k=10) + hold-out final (20%) pour estimation finale.

**Analyses √† produire :**
- Courbe residuals vs fitted (d√©tecter biais non-lin√©aires).  
- Erreurs moyennes par `brand`, par `year` et par `segment` (citadine/SUV/etc.).  
- Distribution des erreurs (boxplots) pour d√©tecter outliers.

---

## 9) Probl√®mes r√©currents & solutions

- **Prix manquant** : supprimer les annonces sans prix ou marquer `price_missing` puis imputer si n√©cessaire.  
- **Prix erron√©s (typos)** : heuristiques (ex. `price < 1000` ou `price > median * 10`) + v√©rification manuelle d‚Äôun √©chantillon.  
- **Donn√©es r√©parties entre pages / formats diff√©rents** : architecture de scrapers modulaires (`one scraper per site`), logs d‚Äôerreurs, tests unitaires pour selecteurs.  
- **Duplicatas** : d√©duplication via cl√© `(title, price, km, year)` et/ou hash des images.  
- **√âlectriques** : si √©chantillon petit ‚Üí entra√Æner mod√®le s√©par√© ; sinon ajouter flag `is_electric`.  
- **V√©hicules sp√©ciaux** (bus, ambulances) : exclure si l‚Äôobjectif est voitures particuli√®res.

---

## 10) S√©parer les jeux de donn√©es

S√©parer ou tagger les subsets suivants pour entra√Ænements et analyses :

- **√âlectriques vs thermiques**  
- **Occasion vs neuf**  
- **Particulier vs professionnel**  
- **Segments** : citadine / berline / SUV / utilitaire / luxe

Ceci permet de comparer performances et comportement des mod√®les selon sous-populations.

---

## 11) D√©ploiement & livraison

- **Prototype UI** : Streamlit ‚Äî rapide √† monter (inputs ‚Üí pr√©diction).  
- **API** : FastAPI (recommand√©) ou Flask pour exposer un endpoint `/predict` (POST JSON).  
- **Docker** : Dockerfile pour containeriser l‚Äôapplication (API ou Streamlit).  
- **Monitoring** : journalisation des appels et des features, stockage des pr√©dictions vs ventes r√©elles pour r√©entra√Ænement et d√©rive du mod√®le.

---

## 12) Plan de travail (sprint simple)

> Optionnel mais pratique pour organiser le travail.

- **Semaine 1** : Choix sites + inspection HTML + impl√©menter 1er scraper (2 sites) ‚Üí collecter ~500 annonces.  
- **Semaine 2** : Nettoyage des donn√©es, EDA, features de base, baseline lin√©aire.  
- **Semaine 3** : Mod√®les avanc√©s (XGBoost / CatBoost / LightGBM), hyperparam tuning.  
- **Semaine 4** : Prototype Streamlit / API FastAPI + Docker + documentation + rapport final.

---

## 13) Exemples d‚Äôexpressions / regex utiles pour parser

**Prix**  
```python
m = re.search(r'(\d[\d\s]*)\s*(dt|tnd|dinar)', text, re.I)
if m:
    price = int(m.group(1).replace(' ', ''))
```
# üìå Extras : Regex & bonnes pratiques (√† coller dans README.md)

## Ann√©e

```python
import re

m = re.search(r'(\d{4})', text)
if m:
    year = int(m.group(1))  # v√©rifier 1980 <= year <= 2025
```
## Kilometrage
```python

import re

m = re.search(r'(\d[\d.,]*)\s*(km|kilom)', text, re.I)
if m:
    km = int(m.group(1).replace('.', '').replace(',', ''))

```
## Premi√®re main (seller_type)
```python
desc = description.lower()
if '1√®re main' in desc or 'premiere main' in desc or 'premi√®re main' in desc:
    seller_type = 'first_owner'
elif 'concessionnaire' in desc or 'garantie' in desc:
    seller_type = 'dealer'
else:
    seller_type = 'private'

```
## 14) Bonnes pratiques & recommandations finales

- **D√©marrer petit** : 500‚Äì1000 annonces pour prototypage.  
- **Modularit√©** : un scraper par site + fonctions utilitaires partag√©es.  
- **Tra√ßabilit√©** : stocker `source_site`, `scrape_date`, `ad_id`, √©ventuellement `raw_html`.  
- **Tests automatis√©s** : assertions sur les selecteurs, tests d‚Äôint√©gration pour scrapers.  
- **Respect l√©gal / √©thique** : v√©rifier `robots.txt`, √©viter le scraping agressif.  
- **Features externes** : envisager taux de change, indices √©conomiques, co√ªt moyen local pour am√©liorer le mod√®le.  

---

## Structure de d√©p√¥t recommand√©e
```
car-price-prediction/
‚îú‚îÄ data/
‚îÇ  ‚îú‚îÄ raw/          # raw html / raw JSON
‚îÇ  ‚îú‚îÄ interim/
‚îÇ  ‚îî‚îÄ processed/    # cleaned CSV / parquet
‚îú‚îÄ notebooks/
‚îÇ  ‚îî‚îÄ 01_EDA_and_modeling.ipynb
‚îú‚îÄ src/
‚îÇ  ‚îú‚îÄ scraping/
‚îÇ  ‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ  ‚îÇ  ‚îú‚îÄ automobile_tn.py
‚îÇ  ‚îÇ  ‚îú‚îÄ sayarti_tn.py
‚îÇ  ‚îÇ  ‚îî‚îÄ utils.py
‚îÇ  ‚îú‚îÄ preprocessing/
‚îÇ  ‚îÇ  ‚îî‚îÄ clean.py
‚îÇ  ‚îú‚îÄ features/
‚îÇ  ‚îÇ  ‚îî‚îÄ build_features.py
‚îÇ  ‚îú‚îÄ models/
‚îÇ  ‚îÇ  ‚îú‚îÄ train.py
‚îÇ  ‚îÇ  ‚îî‚îÄ predict.py
‚îÇ  ‚îî‚îÄ api/
‚îÇ     ‚îî‚îÄ app.py     # FastAPI or Streamlit app
‚îú‚îÄ models/
‚îÇ  ‚îî‚îÄ model.pkl
‚îú‚îÄ Dockerfile
‚îú‚îÄ requirements.txt
‚îî‚îÄ README.md
```
